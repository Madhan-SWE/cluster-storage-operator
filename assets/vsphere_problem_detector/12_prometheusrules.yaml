apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: vsphere-problem-detector
  namespace: openshift-cluster-storage-operator
  labels:
    role: alert-rules
spec:
  groups:
    - name: vsphere-problem-detector.rules
      rules:
      - alert: VSphereOpenshiftNodeHealthFail
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr:  min_over_time(vsphere_node_check_errors[5m]) == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          message: "VSphere health check {{ $labels.check }} is failing on {{ $labels.node }}."
      - alert: VSphereOpenshiftClusterHealthFail
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: min_over_time(vsphere_cluster_check_errors[5m]) == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          message: "VSphere cluster health checks are failing with {{ $labels.check }}"
      - alert: VSphereOpenshiftConnectionFailure
        # Using min_over_time to make sure the metric is `1` for whole 15 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: min_over_time(vsphere_sync_errors[15m]) == 1
        for: 60m
        labels:
          severity: warning
        annotations:
          summary: "vsphere-problem-detector is unable to connect to vSphere vCenter."
          description: |
            vsphere-problem-detector cannot access vCenter. As consequence, other OCP components,
            such as storage or machine API, may not be able to access vCenter too and provide
            their services. Detailed error message can be found in Available condition of
            ClusterOperator "storage", either in console
            (Administration -> Cluster settings -> Cluster operators tab -> storage) or on
            command line: oc get clusteroperator storage -o jsonpath='{.status.conditions[?(@.type=="Available")].message}'
      - alert: CSIWithOldVSphereHWVersion
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        # Using ON() to have logical "and" with unrelated labels.
        expr: |
          min_over_time(vsphere_node_hw_version_total{hw_version=~"vmx-(11|12|13|14)"}[5m]) > 0
          and ON()
          count(cluster_feature_set{name="TechPreviewNoUpgrade"}) > 0
        for: 60m
        labels:
          severity: info
        annotations:
          summary: "Detected vSphere VM with HW version lower than 15, which is not supported by the installed vSphere CSI driver."
          description: |
            The cluster runs the vSphere CSI driver (it has TechPreviewNoUpgrade features enabled) and the CSI driver does not
            support vSphere VMs with HW version lower than 15. Please update HW version of all VMs that are part of the cluster
            to at least HW version 15.
